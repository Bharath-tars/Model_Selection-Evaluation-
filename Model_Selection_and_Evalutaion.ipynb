{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN6GjI2J1UNOtfGucsUGOSU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bharath-tars/Model_Selection-Evaluation-/blob/main/Model_Selection_and_Evalutaion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6rZisKAxMk4"
      },
      "outputs": [],
      "source": [
        "# for array computations and loading data\n",
        "import numpy as np\n",
        "\n",
        "# for building linear regression models and preparing data\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# for building and training neural networks\n",
        "import tensorflow as tf\n",
        "\n",
        "# custom functions\n",
        "# reduce display precision on numpy arrays\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "# suppress warnings\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "tf.autograph.set_verbosity(0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = np.loadtxt('vertopal.com_c2w3_lab2_data1.ipynb', delimiter=',')\n",
        "\n",
        "# Split the inputs and outputs into separate arrays\n",
        "x = data[:,0]\n",
        "y = data[:,1]\n",
        "\n",
        "# Convert 1-D arrays into 2-D because the commands later will require it\n",
        "x = np.expand_dims(x, axis=1)\n",
        "y = np.expand_dims(y, axis=1)\n",
        "\n",
        "print(f\"the shape of the inputs x is: {x.shape}\")\n",
        "print(f\"the shape of the targets y is: {y.shape}\")"
      ],
      "metadata": {
        "id": "rFtLcMR7yUgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get 60% of the dataset as the training set. Put the remaining 40% in temporary variables: x_ and y_.\n",
        "x_train, x_, y_train, y_ = train_test_split(x, y, test_size=0.40, random_state=1)\n",
        "\n",
        "# Split the 40% subset above into two: one half for cross validation and the other for the test set\n",
        "x_cv, x_test, y_cv, y_test = train_test_split(x_, y_, test_size=0.50, random_state=1)\n",
        "\n",
        "# Delete temporary variables\n",
        "del x_, y_\n",
        "\n",
        "print(f\"the shape of the training set (input) is: {x_train.shape}\")\n",
        "print(f\"the shape of the training set (target) is: {y_train.shape}\\n\")\n",
        "print(f\"the shape of the cross validation set (input) is: {x_cv.shape}\")\n",
        "print(f\"the shape of the cross validation set (target) is: {y_cv.shape}\\n\")\n",
        "print(f\"the shape of the test set (input) is: {x_test.shape}\")\n",
        "print(f\"the shape of the test set (target) is: {y_test.shape}\")"
      ],
      "metadata": {
        "id": "6ddIWOv00kP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the class\n",
        "scaler_linear = StandardScaler()\n",
        "\n",
        "# Compute the mean and standard deviation of the training set then transform it\n",
        "X_train_scaled = scaler_linear.fit_transform(x_train)\n",
        "\n",
        "print(f\"Computed mean of the training set: {scaler_linear.mean_.squeeze():.2f}\")\n",
        "print(f\"Computed standard deviation of the training set: {scaler_linear.scale_.squeeze():.2f}\")\n",
        "\n",
        "# Plot the results\n",
        "utils.plot_dataset(x=X_train_scaled, y=y_train, title=\"scaled input vs. target\")"
      ],
      "metadata": {
        "id": "BAAngUod6ZKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the class\n",
        "linear_model = LinearRegression()\n",
        "\n",
        "# Train the model\n",
        "linear_model.fit(X_train_scaled, y_train )"
      ],
      "metadata": {
        "id": "cWQA1Qma6gIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feed the scaled training set and get the predictions\n",
        "yhat = linear_model.predict(X_train_scaled)\n",
        "\n",
        "# Use scikit-learn's utility function and divide by 2\n",
        "print(f\"training MSE (using sklearn function): {mean_squared_error(y_train, yhat) / 2}\")\n",
        "\n",
        "# for-loop implementation\n",
        "total_squared_error = 0\n",
        "\n",
        "for i in range(len(yhat)):\n",
        "    squared_error_i  = (yhat[i] - y_train[i])**2\n",
        "    total_squared_error += squared_error_i                                              \n",
        "\n",
        "mse = total_squared_error / (2*len(yhat))\n",
        "print(f\"training MSE (for-loop implementation): {mse.squeeze()}\")"
      ],
      "metadata": {
        "id": "vt5IMHbh67og"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LINEAR REGRESSION MODEL SELECTION"
      ],
      "metadata": {
        "id": "H8y9xgxcAex1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add polynomial features\n",
        "degree = 1\n",
        "poly = PolynomialFeatures(degree, include_bias=False)\n",
        "X_train_mapped = poly.fit_transform(x_train)\n",
        "X_cv_mapped = poly.transform(x_cv)\n",
        "X_test_mapped = poly.transform(x_test)"
      ],
      "metadata": {
        "id": "PI68A9fE-Ylx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale the features using the z-score\n",
        "scaler = StandardScaler()\n",
        "X_train_mapped_scaled = scaler.fit_transform(X_train_mapped)\n",
        "X_cv_mapped_scaled = scaler.transform(X_cv_mapped)\n",
        "X_test_mapped_scaled = scaler.transform(X_test_mapped)"
      ],
      "metadata": {
        "id": "ayAAlS-3-ZO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize lists that will contain the errors for each model\n",
        "nn_train_mses = []\n",
        "nn_cv_mses = []\n",
        "\n",
        "# Build the models\n",
        "nn_models = utils.build_models()\n",
        "print(len(nn_models))\n",
        "# Loop over the the models\n",
        "for model in nn_models:\n",
        "    \n",
        "    # Setup the loss and optimizer\n",
        "    model.compile(\n",
        "    loss='mse',\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),\n",
        "    )\n",
        "\n",
        "    print(f\"Training {model.name}...\")\n",
        "    \n",
        "    # Train the model\n",
        "    model.fit(\n",
        "        X_train_mapped_scaled, y_train,\n",
        "        epochs=300,\n",
        "        verbose=0\n",
        "    )\n",
        "    \n",
        "    print(\"Done!\\n\")\n",
        "\n",
        "    \n",
        "    # Record the training MSEs\n",
        "    yhat = model.predict(X_train_mapped_scaled)\n",
        "    train_mse = mean_squared_error(y_train, yhat) / 2\n",
        "    nn_train_mses.append(train_mse)\n",
        "    \n",
        "    # Record the cross validation MSEs \n",
        "    yhat = model.predict(X_cv_mapped_scaled)\n",
        "    cv_mse = mean_squared_error(y_cv, yhat) / 2\n",
        "    nn_cv_mses.append(cv_mse)\n",
        "\n",
        "    \n",
        "# print results\n",
        "print(\"RESULTS:\")\n",
        "for model_num in range(len(nn_train_mses)):\n",
        "    print(\n",
        "        f\"Model {model_num+1}: Training MSE: {nn_train_mses[model_num]:.2f}, \" +\n",
        "        f\"CV MSE: {nn_cv_mses[model_num]:.2f}\"\n",
        "        )"
      ],
      "metadata": {
        "id": "rVGtBDJH_5-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the model with the lowest CV MSE\n",
        "model_num = 3\n",
        "\n",
        "# Compute the test MSE\n",
        "yhat = nn_models[model_num-1].predict(X_test_mapped_scaled)\n",
        "test_mse = mean_squared_error(y_test, yhat) / 2\n",
        "\n",
        "print(f\"Selected Model: {model_num}\")\n",
        "print(f\"Training MSE: {nn_train_mses[model_num-1]:.2f}\")\n",
        "print(f\"Cross Validation MSE: {nn_cv_mses[model_num-1]:.2f}\")\n",
        "print(f\"Test MSE: {test_mse:.2f}\")"
      ],
      "metadata": {
        "id": "sDgX2Z62AM7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CLASSIFICATION MODEL SELECTION"
      ],
      "metadata": {
        "id": "GDxAgNlOAll9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Get 60% of the dataset as the training set. Put the remaining 40% in temporary variables.\n",
        "x_bc_train, x_, y_bc_train, y_ = train_test_split(x_bc, y_bc, test_size=0.40, random_state=1)\n",
        "\n",
        "# Split the 40% subset above into two: one half for cross validation and the other for the test set\n",
        "x_bc_cv, x_bc_test, y_bc_cv, y_bc_test = train_test_split(x_, y_, test_size=0.50, random_state=1)\n",
        "\n",
        "# Delete temporary variables\n",
        "del x_, y_\n",
        "\n",
        "print(f\"the shape of the training set (input) is: {x_bc_train.shape}\")\n",
        "print(f\"the shape of the training set (target) is: {y_bc_train.shape}\\n\")\n",
        "print(f\"the shape of the cross validation set (input) is: {x_bc_cv.shape}\")\n",
        "print(f\"the shape of the cross validation set (target) is: {y_bc_cv.shape}\\n\")\n",
        "print(f\"the shape of the test set (input) is: {x_bc_test.shape}\")\n",
        "print(f\"the shape of the test set (target) is: {y_bc_test.shape}\")"
      ],
      "metadata": {
        "id": "z6Yl0ShICH6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize lists that will contain the errors for each model\n",
        "nn_train_error = []\n",
        "nn_cv_error = []\n",
        "\n",
        "# Build the models\n",
        "models_bc = utils.build_models()\n",
        "\n",
        "# Loop over each model\n",
        "for model in models_bc:\n",
        "    \n",
        "    # Setup the loss and optimizer\n",
        "    model.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
        "    )\n",
        "\n",
        "    print(f\"Training {model.name}...\")\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(\n",
        "        x_bc_train, y_bc_train,\n",
        "        epochs=200,\n",
        "        verbose=0\n",
        "    )\n",
        "    \n",
        "    print(\"Done!\\n\")\n",
        "    \n",
        "    # Set the threshold for classification\n",
        "    threshold = 0.5\n",
        "    \n",
        "    # Record the fraction of misclassified examples for the training set\n",
        "    yhat = model.predict(x_bc_train)\n",
        "    yhat = tf.math.sigmoid(yhat)\n",
        "    yhat = np.where(yhat >= threshold, 1, 0)\n",
        "    train_error = np.mean(yhat != y_bc_train)\n",
        "    nn_train_error.append(train_error)\n",
        "\n",
        "    # Record the fraction of misclassified examples for the cross validation set\n",
        "    yhat = model.predict(x_bc_cv)\n",
        "    yhat = tf.math.sigmoid(yhat)\n",
        "    yhat = np.where(yhat >= threshold, 1, 0)\n",
        "    cv_error = np.mean(yhat != y_bc_cv)\n",
        "    nn_cv_error.append(cv_error)\n",
        "\n",
        "# Print the result\n",
        "for model_num in range(len(nn_train_error)):\n",
        "    print(\n",
        "        f\"Model {model_num+1}: Training Set Classification Error: {nn_train_error[model_num]:.5f}, \" +\n",
        "        f\"CV Set Classification Error: {nn_cv_error[model_num]:.5f}\"\n",
        "        )"
      ],
      "metadata": {
        "id": "p9Gd_kvXAthm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the model with the lowest error\n",
        "model_num = 2\n",
        "\n",
        "# Compute the test error\n",
        "yhat = models_bc[model_num-1].predict(x_bc_test)\n",
        "yhat = tf.math.sigmoid(yhat)\n",
        "yhat = np.where(yhat >= threshold, 1, 0)\n",
        "nn_test_error = np.mean(yhat != y_bc_test)\n",
        "\n",
        "print(f\"Selected Model: {model_num}\")\n",
        "print(f\"Training Set Classification Error: {nn_train_error[model_num-1]:.4f}\")\n",
        "print(f\"CV Set Classification Error: {nn_cv_error[model_num-1]:.4f}\")\n",
        "print(f\"Test Set Classification Error: {nn_test_error:.4f}\")"
      ],
      "metadata": {
        "id": "v3RyCXf7CiM0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}